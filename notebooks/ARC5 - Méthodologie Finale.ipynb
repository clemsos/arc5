{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARC5 - Parse data as network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "from slugify import slugify\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "data_dir = os.getcwd()\n",
    "fichier_theses = \"../final/ARC5-Final-ADRs.csv\"\n",
    "fichier_projets = \"../final/ARC5-Final-projets.csv\"\n",
    "fichier_partenaires= \"../final/partenaires.csv\"\n",
    "partner_categories_file=\"../final/partner_categories.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_node(name, type, start, end, tmp=False) : \n",
    "    \n",
    "    slug = slugify(name.decode('utf-8')) + \"-\" + type\n",
    "    if tmp : slug = slug + \"-tmp\"\n",
    "    \n",
    "    try :\n",
    "        if start > G.node[slug][\"start\"] : start =  G.node[slug][\"start\"]\n",
    "        if end > G.node[slug][\"end\"] : start =  G.node[slug][\"end\"]\n",
    "    except:\n",
    "        start = start\n",
    "        endd = end\n",
    "            \n",
    "    node = {}\n",
    "    node[\"id\"] = slug\n",
    "    node[\"type\"] = type\n",
    "    node[\"name\"] = name\n",
    "    node[\"start\"] = start\n",
    "    node[\"end\"] = end\n",
    "    node[\"tmp\"] = tmp\n",
    "    \n",
    "    G.add_node(node[\"id\"], node)\n",
    "    return node[\"id\"]\n",
    "\n",
    "# keep data when merging edges\n",
    "def merge_edge_data(Graph, edge, data):\n",
    "    \n",
    "    try : \n",
    "        Graph.edge[e[0]][e[1]]    \n",
    "    except KeyError:\n",
    "        Graph.add_edge(e[0], e[1])\n",
    "        \n",
    "    try:\n",
    "        prevData = Graph.edge[e[0]][e[1]][\"additionalInfo\"]\n",
    "        Graph.edge[e[0]][e[1]][\"additionalInfo\"] = prevData + data\n",
    "    except KeyError:\n",
    "        Graph.edge[e[0]][e[1]][\"additionalInfo\"] = data\n",
    "    \n",
    "    x = Graph.edge[e[0]][e[1]][\"additionalInfo\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse thèses et projets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../final/ARC5-Final-ADRs.csv\n",
      "../final/ARC5-Final-projets.csv\n",
      "338 nodes\n",
      "680 edges\n",
      "Counter({'personne': 111, 'projet': 75, 'laboratoire': 61, 'these': 49, 'etablissement': 22, 'ecole-doctorale': 16, 'ville': 4})\n"
     ]
    }
   ],
   "source": [
    "print fichier_theses\n",
    "print fichier_projets\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "with open( os.path.join(data_dir, fichier_projets), \"r\") as f :\n",
    "    reader = csv.DictReader(f)\n",
    "    for line in reader :\n",
    "\n",
    "        start = int(line[\"Start\"])\n",
    "        end =  start+3\n",
    "\n",
    "        projet = create_node(line[\"Titre\"], \"projet\", start, end)\n",
    "        etablissement = create_node(line[\"Etablissement\"], \"etablissement\", start, end)\n",
    "        porteur = create_node(line[\"Porteur (Nom)\"] + \" \" + line[\"Porteur(Prenom)\"], \"personne\", start, end)\n",
    "        laboratoire = create_node(line[\"Laboratoire\"], \"laboratoire\", start, end)\n",
    "        ville = create_node(line[\"Ville\"], \"ville\", start, end)\n",
    "        \n",
    "        edges = []\n",
    "        edges.append((projet, porteur))\n",
    "        edges.append((projet, etablissement))\n",
    "        edges.append((projet, laboratoire))\n",
    "        edges.append((etablissement, ville))\n",
    "        edges.append((laboratoire, ville))\n",
    "        edges.append((laboratoire, porteur))\n",
    "        \n",
    "        for e in edges : merge_edge_data(G, e, \"* **projet** : %s \\n \"%line[\"Titre\"])\n",
    "\n",
    "with open( os.path.join(data_dir, fichier_theses), \"r\") as f :\n",
    "    reader = csv.DictReader(f)\n",
    "    for line in reader :\n",
    "        \n",
    "        start = int(line[\"Start\"])\n",
    "        end =  start+3\n",
    "\n",
    "        these = create_node(line[\"Titre\"], \"these\", start, end)\n",
    "        etablissement = create_node(line[\"Etablissement\"], \"etablissement\", start, end)\n",
    "        directeur = create_node(line[\"Directeur de thèse\"], \"personne\", start, end)\n",
    "        ecole_doctorale = create_node(line[\"Ecole doctorale\"], \"ecole-doctorale\", start, end)\n",
    "        laboratoire = create_node(line[\"Laboratoire\"], \"laboratoire\", start, end)\n",
    "        \n",
    "        edges = []\n",
    "        edges.append((these, etablissement))\n",
    "        edges.append((these, laboratoire))\n",
    "        edges.append((these, directeur))\n",
    "        edges.append((these, ecole_doctorale))\n",
    "        edges.append((ecole_doctorale, ville))\n",
    "        edges.append((laboratoire, ville))\n",
    "        edges.append((laboratoire, directeur))\n",
    "        edges.append((etablissement, ville))\n",
    "        \n",
    "        for e in edges : merge_edge_data(G, e, \"* **thèse** : %s \\n \"%line[\"Titre\"])\n",
    "\n",
    "        \n",
    "print \"%s nodes\"%len(G.nodes())\n",
    "print \"%s edges\"%len(G.edges())\n",
    "print Counter([n[1][\"type\"] for n in G.nodes(data=True)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse partenaires "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../final/partenaires.csv\n",
      "602 nodes\n",
      "1070 edges\n"
     ]
    }
   ],
   "source": [
    "print fichier_partenaires\n",
    "\n",
    "with open( os.path.join(data_dir,  fichier_partenaires), \"r\") as f :\n",
    "    reader = csv.DictReader(f) \n",
    "    for line in reader:\n",
    "        \n",
    "        partenaire = create_node(line[\"Structure\"], \"partenaire\", start, end)\n",
    "        referent = create_node(line[\"Personne référente\"], \"personne\", start, end)\n",
    "        ville = create_node(line[\"Ville\"], \"ville\", start, end)\n",
    "        \n",
    "        # check if the project exists\n",
    "        projet_name = line[\"Projet\"]\n",
    "        projet_id = slugify(projet_name.decode('utf-8'))\n",
    "        try: \n",
    "            projet = G.node[projet_id + \"-these\"][\"id\"]\n",
    "        except KeyError:\n",
    "            try : \n",
    "                projet = G.node[projet_id + \"-projet\"][\"id\"]\n",
    "            except KeyError:\n",
    "                # add projet with a tmp flag to make sure we are not erasing anything\n",
    "                projet = create_node(projet_name, \"projet\", start, end, True)\n",
    "            \n",
    "#         edges = itertools.combinations([partenaire, ville, referent, projet], 2)\n",
    "        edges = []\n",
    "        edges.append((partenaire, ville))\n",
    "        edges.append((partenaire, referent))\n",
    "        edges.append((partenaire, projet))\n",
    "        \n",
    "        for e in edges : \n",
    "            merge_edge_data(G, e, \"* **projet** : %s \\n \"%projet_name)\n",
    "\n",
    "# remove empty node\n",
    "# G.nodes().remove(\"\")\n",
    "print \"%s nodes\"%len(G.nodes())\n",
    "print \"%s edges\"%len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check similar /identical words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702 similar nodes\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# store similar nodes in a file\n",
    "similar_words_file = \"../final/raw_similar_words.txt\"\n",
    "similars = []\n",
    "if os.path.isfile(similar_words_file):\n",
    "    with open(similar_words_file, \"r\") as infile:\n",
    "        similars = pickle.load(infile)\n",
    "        \n",
    "# get all similar nodes\n",
    "if len(similars) == 0:\n",
    "    for node in G.nodes() :\n",
    "        similar = get_close_matches(node, G.nodes())\n",
    "        if len(similar) > 0 :\n",
    "            for s in similar:\n",
    "                if G.node[node][\"type\"] == G.node[s][\"type\"] and len( set([node,s]) ) != 1: # check if has the same type\n",
    "                    if set([node,s]) not in similars : # check if it is already added\n",
    "                        if s != \"\" :\n",
    "                            similars.append(set([node,s]))\n",
    "\n",
    "if not os.path.isfile(similar_words_file):\n",
    "    with open(similar_words_file, \"wb\") as outfile:\n",
    "        pickle.dump(similars, outfile)\n",
    "\n",
    "print \"%s similar nodes\"%len(similars)\n",
    "\n",
    "# print similars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 duplicate nodes loaded\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "matches = []\n",
    "\n",
    "if os.path.isfile(\"../final/matches.txt\"):\n",
    "    with open(\"matches.txt\", \"r\") as infile:\n",
    "        matches = pickle.load(infile)\n",
    "    print \"%s duplicate nodes loaded\"%len(matches)\n",
    "\n",
    "# print matches\n",
    "if len(matches) == 0:\n",
    "    for i, sim in enumerate(similars):\n",
    "        print \"---------- %s/%s\"%(i, len(similars))\n",
    "        if sim not in matches:\n",
    "            source = tuple(sim)[0]\n",
    "            target = tuple(sim)[1]\n",
    "\n",
    "            # manually check if similar\n",
    "\n",
    "            manual_check = raw_input(\"Are those words similar (y/n)?  \\n '%s' \\n '%s' ?: \"%(source, target))\n",
    "            if manual_check == \"y\":\n",
    "                print \"they are similar\"\n",
    "                matches.append( (source , target) )\n",
    "                with open(\"matches.txt\", \"wb\") as outfile:\n",
    "                    pickle.dump(matches, outfile)\n",
    "            else :\n",
    "                print \"they are not similar\"\n",
    "        else :\n",
    "            print \"already processed\"\n",
    "\n",
    "    print \"%s names that match \"%len(matches)\n",
    "\n",
    "print \"-\"*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check tmp projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 tmp projects\n",
      "[u'pistekne-projet-danimation-en-vue-de-la-creation-dun-gdr-cnrs-autour-des-processus-de-creation-production-dans-les-rapports-arts-sciences-techniques-projet-tmp', u'-projet-tmp', u'monnaie-imperiale-et-corpus-numismatique-en-rhone-alpes-mara-postdoc-projet-tmp']\n",
      "epistekne-projet-danimation-en-vue-de-la-creation-dun-gdr-cnrs-autour-des-processus-de-creation-production-dans-les-rapports-arts-sciences-techniques-projet\n",
      "mara-monnaie-antique-en-rhone-alpes-du-document-monetaire-a-son-exploitation-projet\n",
      "monnaie-imperiale-et-corpus-numismatique-en-rhone-alpes-projet\n",
      "monnaie-imperiale-et-corpus-numismatique-en-rhone-alpes-mara-postdoc-projet-tmp\n",
      "7 occurences matches\n"
     ]
    }
   ],
   "source": [
    "tmp_projects = [ n[0] for n in G.nodes(data=True) if n[1][\"type\"] == \"projet\" and n[1][\"tmp\"] == True ]\n",
    "print \"%s tmp projects\"%len(tmp_projects)\n",
    "\n",
    "print tmp_projects\n",
    "for p in [ n[0] for n in G.nodes(data=True) if n[1][\"type\"] == \"projet\"]:\n",
    "    if \"monnaie\" in p : print p\n",
    "    if \"epistekne\" in p : print p\n",
    "\n",
    "matches.append((\n",
    "    \"monnaie-imperiale-et-corpus-numismatique-en-rhone-alpes-projet\", \n",
    "    \"monnaie-imperiale-et-corpus-numismatique-en-rhone-alpes-mara-postdoc-projet-tmp\"\n",
    ")) \n",
    "\n",
    "matches.append((\n",
    "    \"monnaie-imperiale-et-corpus-numismatique-en-rhone-alpes-projet\", \n",
    "    \"mara-monnaie-antique-en-rhone-alpes-du-document-monetaire-a-son-exploitation-projet\"\n",
    ")) \n",
    "\n",
    "matches.append((\n",
    "    \"epistekne-projet-danimation-en-vue-de-la-creation-dun-gdr-cnrs-autour-des-processus-de-creation-production-dans-les-rapports-arts-sciences-techniques-projet\",\n",
    "    \"pistekne-projet-danimation-en-vue-de-la-creation-dun-gdr-cnrs-autour-des-processus-de-creation-production-dans-les-rapports-arts-sciences-techniques-projet-tmp\"\n",
    "))\n",
    "\n",
    "print \"%s occurences matches\"%len(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge matching nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 duplicates nodes\n"
     ]
    }
   ],
   "source": [
    "# prevent deleting indexes\n",
    "match_reverse_index = { m[1]:m[0] for m in matches }\n",
    "\n",
    "for k in match_reverse_index.values():\n",
    "    if k in match_reverse_index.keys() :\n",
    "        matches.append((match_reverse_index[k], k)) \n",
    "\n",
    "print \"%s duplicates nodes\"%len(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a clean graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1070 1070\n",
      "-5 nodes merged\n",
      "0 edges merged\n",
      "----------\n",
      "REMOVE DUPLICATES\n",
      "before : 602 nodes / 1070 edges \n",
      "after : 602 nodes / 1075 edges \n",
      "----------\n",
      "types :Counter({'personne': 225, 'partenaire': 114, 'projet': 78, 'laboratoire': 61, 'these': 49, 'ville': 37, 'etablissement': 22, 'ecole-doctorale': 16}) \n",
      "dates :Counter({2015: 304, 2013: 129, 2012: 102, 2014: 29, 2011: 27, 2016: 6, 2017: 5}) \n",
      "----------\n",
      "3 tmp projects\n",
      "[u'-projet-tmp', u'pistekne-projet-danimation-en-vue-de-la-creation-dun-gdr-cnrs-autour-des-processus-de-creation-production-dans-les-rapports-arts-sciences-techniques-projet-tmp', u'monnaie-imperiale-et-corpus-numismatique-en-rhone-alpes-mara-postdoc-projet-tmp']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "excluded_nodes = [\"\"]\n",
    "duplicates = { m[0] : m[1] for m in matches}\n",
    "\n",
    "edges_count_before = len(G.edges())\n",
    "nodes_count_before = len(G.nodes())\n",
    "\n",
    "G_clean = G.copy()\n",
    "# G_clean.remove_node(\"\") # delete the node with an empty name  \n",
    "print edges_count_before, len(G_clean.edges())\n",
    "\n",
    "# get only clean nodes and edges\n",
    "for n in G_clean.nodes(data=True):\n",
    "    if n[0] not in excluded_nodes:\n",
    "        \n",
    "        if n[0] in duplicates.keys():\n",
    "            \n",
    "            # get the duplicate\n",
    "            duplicate_id = duplicates[n[0]]\n",
    "            \n",
    "            # store information from the previous node\n",
    "            duplicate = G_clean.node[duplicate_id]\n",
    "            \n",
    "            if duplicate[\"start\"] != \"\" and duplicate[\"start\"] < n[1][\"start\"]:\n",
    "                G_clean.node[n[0]][\"start\"] = duplicate[\"start\"]\n",
    "            \n",
    "            if duplicate[\"end\"] != \"\" and duplicate[\"end\"] > n[1][\"end\"]:\n",
    "                G_clean.node[n[0]][\"end\"] = duplicate[\"end\"]\n",
    "        \n",
    "            # update edges\n",
    "            duplicate_edges = G_clean.edges(duplicate_id)\n",
    "            for e in duplicate_edges: \n",
    "                G_clean.add_edge( n[0], e[1])\n",
    "\n",
    "\n",
    "# delete the duplicates nodes and all their edges\n",
    "# for d in duplicates : \n",
    "#     G_clean.remove_node(d)\n",
    "\n",
    "print \"%s nodes merged\"%(edges_count_before-len(G_clean.edges()))\n",
    "print \"%s edges merged\"%(nodes_count_before-len(G_clean.nodes()))\n",
    "print \"-\"*10\n",
    "\n",
    "print \"REMOVE DUPLICATES\"\n",
    "print \"before : %s nodes / %s edges \"%(len(G.nodes()), len(G.edges()))\n",
    "print \"after : %s nodes / %s edges \"%(len(G_clean.nodes()), len(G_clean.edges()))\n",
    "print \"-\"*10\n",
    "\n",
    "print \"types :%s \" % Counter([ n[1][\"type\"] for n in G_clean.nodes(data=True) ])\n",
    "print \"dates :%s \" % Counter([ n[1][\"start\"] for n in G_clean.nodes(data=True) ])\n",
    "print \"-\"*10\n",
    "\n",
    "tmp_projects = [ n[0] for n in G_clean.nodes(data=True) if n[1][\"type\"] == \"projet\" and n[1][\"tmp\"] == True ]\n",
    "print \"%s tmp projects\"%len(tmp_projects)\n",
    "print tmp_projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert persons to edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before : 602 nodes / 1075 edges\n",
      "after : 377 nodes / 896 edges\n"
     ]
    }
   ],
   "source": [
    "print \"before : %s nodes / %s edges\"%(len(G_clean.nodes()),len(G_clean.edges()))\n",
    "\n",
    "G_without_people = G_clean.copy()\n",
    "\n",
    "# get all persons in the graph\n",
    "persons = [node[0] for node in G_without_people.nodes(data=True) if node[1][\"type\"] == \"personne\"]\n",
    "# persons_edges = clean_G.edges(persons, data=True)\n",
    "\n",
    "\n",
    "for person in persons:\n",
    "\n",
    "    # edges for a single person\n",
    "    person_edges = G_without_people.edges(person)\n",
    "  \n",
    "    # get all nodes linked by a single person\n",
    "    list_of_person_nodes = []; map(list_of_person_nodes.extend, map(list,person_edges))\n",
    "    assert len(list_of_person_nodes) == len(person_edges)*2 # make sure we have all nodes\n",
    "    \n",
    "    clean_nodes = [n for n in list_of_person_nodes if n != person]\n",
    "    #  assert len(clean_nodes) == len(person_edges) # make sure we have all new nodes, except the person\n",
    "\n",
    "    if len(person_edges) > 2 : # if have less than degree of 1 then remove node\n",
    "\n",
    "        # get data from the node to add to the edge\n",
    "        data = G_without_people.node[person]\n",
    "\n",
    "        # create new edges between all those\n",
    "        new_edges = list(itertools.combinations(clean_nodes, 2))\n",
    "\n",
    "        # create new edges with merge data info\n",
    "        for e in new_edges:\n",
    "            merge_edge_data(G_without_people, e, \"* **personnel commun**\")\n",
    "\n",
    "    # remove person from the graph\n",
    "    G_without_people.remove_node(person)\n",
    "\n",
    "print \"after : %s nodes / %s edges\"%(len(G_without_people.nodes()),len(G_without_people.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert projects to edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before : 377 nodes / 896 edges\n",
      "after : 250 nodes / 1134 edges\n",
      "Counter({'partenaire': 114, 'laboratoire': 61, 'ville': 37, 'etablissement': 22, 'ecole-doctorale': 16})\n"
     ]
    }
   ],
   "source": [
    "print \"before : %s nodes / %s edges\"%(len(G_without_people.nodes()),len(G_without_people.edges()))\n",
    "\n",
    "G_without_people_and_projects = G_without_people.copy()\n",
    "\n",
    "# get all projects in the graph\n",
    "projects = [node[0] for node in G_without_people_and_projects.nodes(data=True) if node[1][\"type\"] == \"projet\" or node[1][\"type\"] == \"these\" or node[1][\"type\"] == \"postdoc\" ]\n",
    "\n",
    "\n",
    "    \n",
    "for project in projects:\n",
    "\n",
    "    # edges for a single person\n",
    "    project_edges = G_without_people_and_projects.edges(project)\n",
    "  \n",
    "    # get all nodes linked by a single person\n",
    "    list_of_project_nodes = []; map(list_of_project_nodes.extend, map(list, project_edges))\n",
    "    assert len(list_of_project_nodes) == len(project_edges)*2 # make sure we have all nodes\n",
    "    \n",
    "    clean_nodes = [n for n in list_of_project_nodes if n != project]\n",
    "#     assert len(clean_nodes) == len(person_edges) # make sure we have all new nodes, except the person\n",
    "\n",
    "    if len(project_edges) > 2 : # if have less than degree of 1 then remove node\n",
    "\n",
    "        # get data from the node to add to the edge\n",
    "        data = G_without_people_and_projects.node[project]\n",
    "\n",
    "        # create new edges between all those\n",
    "        new_edges = list(itertools.combinations(clean_nodes, 2))\n",
    "                \n",
    "        # TODO: merge data into edge info\n",
    "        for e in new_edges:\n",
    "            merge_edge_data(G_without_people_and_projects, e, \"* **projet** ： %s \\n\"%str(G_clean.node[project][\"name\"]))\n",
    "#             G_without_people_and_projects.add_edge( e[0], e[1], {\"type\" : \"projet\", \"additionalInfo\" : data} )\n",
    "\n",
    "    # remove person from the graph\n",
    "    G_without_people_and_projects.remove_node(project)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "print \"after : %s nodes / %s edges\"%(len(G_without_people_and_projects.nodes()),len(G_without_people_and_projects.edges()))\n",
    "print Counter([ c[1][\"type\"] for c in G_without_people_and_projects.nodes(data=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the list of partners with the right type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 partenaires\n",
      "Counter({'partenaire': 114, 'laboratoire': 61, 'ville': 37, 'etablissement': 22, 'ecole-doctorale': 16})\n",
      "../final/partner_categories.txt\n",
      "289 partner categories loaded\n",
      "0 partner without categories\n",
      "Counter({'laboratoire': 61, 'ville': 37, 'creation': 36, 'm\\xc3\\xa9diation': 36, 'patrimoine': 33, 'etablissement': 22, 'ecole-doctorale': 16, 'cst': 5, 'enseignement': 4})\n"
     ]
    }
   ],
   "source": [
    "partners = [ n for n in G_without_people_and_projects.nodes(data=True) if n[1][\"type\"] == \"partenaire\"]\n",
    "print \"%s partenaires\"%len(partners)\n",
    "print Counter([n[1][\"type\"] for n in G_without_people_and_projects.nodes(data=True)]) \n",
    "\n",
    "categories=[\n",
    "{\n",
    "    \"id\" : \"patrimoine\",\n",
    "    \"name\" : \"Institutions Patrimoniales\"\n",
    "},\n",
    "{\n",
    "    \"id\" : \"creation\",\n",
    "    \"name\" : \"Création\"\n",
    "    },\n",
    "{\n",
    "    \"id\" : \"médiation\",\n",
    "    \"name\" :  \"Structures Médiatrices\"\n",
    "},\n",
    "{\n",
    "    \"id\" : \"cst\",\n",
    "    \"name\" :  \"Culture Scientifique et Technique (CST)\"\n",
    "},\n",
    "{\n",
    "    \"id\" : \"enseignement\",\n",
    "    \"name\" :  \"Enseignement & Recherche\"\n",
    "}\n",
    "]\n",
    "\n",
    "# import existing results\n",
    "partner_categories_file=\"../final/partner_categories.txt\"\n",
    "print partner_categories_file\n",
    "if os.path.isfile(partner_categories_file):\n",
    "    with open(partner_categories_file, \"r\") as infile:\n",
    "        partner_categories = pickle.load(infile)\n",
    "#         print partner_categories_raw\n",
    "#         partner_categories =  { partner_categories_raw[p] for p in partner_categories_raw }\n",
    "    partner_categories_names = [str(p) for p in partner_categories.keys()]\n",
    "print \"%s partner categories loaded\"%len(partner_categories_names )\n",
    "\n",
    "\n",
    "partner_without_categories = [ p for p in partners if p[0] not in partner_categories_names]\n",
    "print \"%s partner without categories\"%len( partner_without_categories )\n",
    "\n",
    "for partner in partner_without_categories:\n",
    "    print \"-\"*10\n",
    "\n",
    "    manual_check = raw_input(\"\"\"A quelle catégorie appartient? \\n\n",
    "    [0] : Institutions Patrimoniales (musées, bibliothèques, archives...)\n",
    "    [1] : Création (théatre, art...)\n",
    "    [2] : Structures Médiatrices (CCSTI, Arald, OPC, Nacre...)\n",
    "    [3] : Culture Scientifique et Technique (CST)\n",
    "    [4] : Enseignement (Ecoles, conservatoire...)\\n\n",
    "    '%s'\n",
    "    \"\"\"%partner[1][\"name\"])\n",
    "\n",
    "    # assert int(manual_check)\n",
    "    assert int(manual_check)  < 5\n",
    "\n",
    "    category  = categories[int(manual_check)]\n",
    "    print \"%s\"%(category[\"name\"])\n",
    "\n",
    "    partner_categories[partner[0]] = category[\"id\"]\n",
    "\n",
    "    with open(partner_categories_file, \"wb\") as outfile:\n",
    "        pickle.dump(partner_categories, outfile)\n",
    "\n",
    "G_ok = G_without_people_and_projects.copy()\n",
    "\n",
    "for n in G_ok.nodes(data=True):\n",
    "    if n[1][\"type\"] == \"partenaire\" : \n",
    "        n[1][\"type\"] = partner_categories[n[0]]\n",
    "\n",
    "print Counter([n[1][\"type\"] for n in G_ok.nodes(data=True)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the final graph on Topogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<topogram_client.TopogramAPIClient object at 0x7f5b73ea1b50>\n",
      "{u'status': u'error', 'status_code': 200, u'message': u'A topogram with the same name already exists', u'data': [{u'sharedPublic': False, u'name': u'ARC 5 - Collaborations Culture / Recherche en Rh\\xf4ne-Alpes', u'owner': u'FWGtG3EXa2F7nZRKp', u'_id': u'3Fep7oZAFjqBnHLQR', u'slug': u'arc-5-collaborations-culture-recherche-en-rhne-alpes', u'createdAt': u'2016-09-12T04:25:24.618Z'}]}\n",
      "0 existing edges, 0 existing nodes\n",
      "nodes deleted\n",
      "edges deleted\n",
      "0 existing edges, 0 existing nodes\n",
      "creating 250 nodes ...\n",
      "creating 1134 edges ...\n",
      "done. Topogram is online at http://app.topogram.io/topograms/3Fep7oZAFjqBnHLQR\n"
     ]
    }
   ],
   "source": [
    "from topogram_client import TopogramAPIClient\n",
    "import logging \n",
    "\n",
    "# passwords\n",
    "TOPOGRAM_URL = \"https://app.topogram.io\" # \"http://localhost:3000\"\n",
    "USER = \"arc5@arc5.com\"\n",
    "PASSWORD = \"culture&recherche\"\n",
    "\n",
    "# connect to the topogram instance \n",
    "topogram = TopogramAPIClient(TOPOGRAM_URL)\n",
    "\n",
    "# topogram.create_user(USER, PASSWORD)\n",
    "topogram.user_login(USER, PASSWORD)\n",
    "print topogram\n",
    "\n",
    "r = topogram.create_topogram(\"ARC 5 - Collaborations Culture / Recherche en Rhône-Alpes\")\n",
    "print r\n",
    "topogram_ID = r[\"data\"][0][\"_id\"]\n",
    "\n",
    "\n",
    "# delete existing nodes\n",
    "existing_nodes = topogram.get_nodes(topogram_ID)[\"data\"]\n",
    "existing_edges = topogram.get_edges(topogram_ID)[\"data\"]\n",
    "print \"%s existing edges, %s existing nodes\"%(len(existing_edges), len(existing_nodes))\n",
    "\n",
    "topogram.delete_nodes([n[\"_id\"] for n in existing_nodes])\n",
    "print \"nodes deleted\"\n",
    "topogram.delete_edges([n[\"_id\"] for n in existing_edges])\n",
    "print \"edges deleted\"\n",
    "\n",
    "existing_nodes = topogram.get_nodes(topogram_ID)[\"data\"]\n",
    "existing_edges = topogram.get_edges(topogram_ID)[\"data\"]\n",
    "print \"%s existing edges, %s existing nodes\"%(len(existing_edges), len(existing_nodes))\n",
    "\n",
    "\n",
    "# create the graph\n",
    "nodes = []\n",
    "for n in G_ok.nodes(data=True): \n",
    "    node = n[1]\n",
    "    node[\"id\"] = n[0]\n",
    "    node[\"group\"] = n[1][\"type\"]\n",
    "    nodes.append(node)\n",
    "\n",
    "print \"creating %s nodes ...\"%len(nodes)\n",
    "r = topogram.create_nodes(topogram_ID, nodes)\n",
    "\n",
    "edges = []\n",
    "for e in G_ok.edges(data=True): \n",
    "    edge = e[2]\n",
    "    edge[\"source\"] = e[0]\n",
    "    edge[\"target\"] = e[1]\n",
    "    edges.append(edge)\n",
    "\n",
    "print \"creating %s edges ...\"%len(edges)\n",
    "r = topogram.create_edges(topogram_ID, edges)\n",
    "\n",
    "print \"done. Topogram is online at http://app.topogram.io/topograms/%s\"%topogram_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
